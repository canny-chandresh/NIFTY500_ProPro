name: Hourly Train & Daily Notify

on:
  schedule:
    # Every market hour (Mon–Fri) at :15 — adjust if you like
    - cron: "15 * * * 1-5"
  workflow_dispatch: {}

jobs:
  run:
    runs-on: ubuntu-latest
    timeout-minutes: 30
    env:
      TZ: Asia/Kolkata
      PYTHONUNBUFFERED: "1"

    steps:
      - name: Checkout
        uses: actions/checkout@v4

      - name: Show tree (sanity)
        run: |
          echo "== repo root =="; ls -la
          echo "== src =="; ls -la src || true
          echo "== workflows =="; ls -la .github/workflows || true

      - name: Setup Python
        uses: actions/setup-python@v5
        with:
          python-version: "3.11"

      - name: Install deps (lean daytime)
        run: |
          python -m pip install --upgrade pip
          pip install pandas numpy scikit-learn pyarrow fastparquet yfinance ta
          # news sentiment (optional but enabled now)
          pip install feedparser vaderSentiment || true

      # ---------- DATA INGEST ----------
      - name: Ingest (daily + intraday today + macro)
        run: |
          python - <<'PY'
          import sys; sys.path.append("src")
          from config import CONFIG
          import data_ingest as di
          uni = CONFIG.get("universe", [])
          out = di.run_all(uni)
          print("INGEST_SUMMARY:", out)
          PY

      # ---------- NEWS SENTIMENT (hourly) ----------
      - name: News sentiment (hourly)
        run: |
          python - <<'PY'
          import sys; sys.path.append("src")
          try:
            from discovery.extractors.news_extractor import fetch
            print(fetch())
          except Exception as e:
            print("news_extractor skipped / soft-fail:", e)
          PY

      # ---------- FAST ALPHAS (hourly-safe) ----------
      - name: Compute fast alphas
        run: |
          python - <<'PY'
          import sys; sys.path.append("src")
          import feature_store
          from alpha.runtime import run_enabled_alphas
          from config import CONFIG
          ff = feature_store.get_feature_frame(CONFIG.get("universe", []))
          out = run_enabled_alphas(ff, fast_only=True)
          print({"alpha_cols": [c for c in out.columns if c.startswith("alpha_")]})
          PY

      # ---------- MAIN PIPELINE (lean) ----------
      - name: Run pipeline (safe wrappers)
        env:
          TG_BOT_TOKEN: ${{ secrets.TG_BOT_TOKEN }}
          TG_CHAT_ID:  ${{ secrets.TG_CHAT_ID }}
        run: |
          python - <<'PY'
          import sys, traceback; sys.path.append("src")
          from pathlib import Path

          def safe(label, fn):
              print(f">> {label}")
              try:
                  fn(); print(f"<< {label} OK")
              except Exception as e:
                  print(f"<< {label} ERROR: {e}")
                  traceback.print_exc()

          try:
              from entrypoints import daily_update, eod_task, periodic_reports_task, after_run_housekeeping
              from pipeline import run_paper_session
          except Exception as e:
              print("Import error:", e); traceback.print_exc(); raise SystemExit(0)

          safe("daily_update()", lambda: daily_update())
          safe("run_paper_session(top_k=5)", lambda: run_paper_session(top_k=5))
          # EOD and periodic are internally time-gated (15:15 IST window, etc.)
          safe("eod_task()", lambda: eod_task())
          safe("periodic_reports_task()", lambda: periodic_reports_task())
          safe("after_run_housekeeping()", lambda: after_run_housekeeping())
          raise SystemExit(0)
          PY

      # ---------- ARTIFACTS (logs & reports) ----------
      - name: Upload debug logs (always)
        if: always()
        uses: actions/upload-artifact@v4
        with:
          name: debug_hourly
          path: |
            reports/debug/**
            reports/eod/**
            reports/periodic/**
            datalake/**
          if-no-files-found: ignore
