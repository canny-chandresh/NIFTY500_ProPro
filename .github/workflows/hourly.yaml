name: Hourly Train & Daily Notify

on:
  schedule:
    - cron: "15 * * * 1-5"     # every weekday at :15
  workflow_dispatch: {}         # manual "Run workflow" button

jobs:
  run:
    runs-on: ubuntu-latest
    timeout-minutes: 30

    steps:
      - name: Checkout
        uses: actions/checkout@v4

      # ---------- Ensure datalake exists (unzip once if needed) ----------
      - name: Ensure datalake exists (unpack once)
        run: |
          if [ -d "datalake" ]; then
            echo "datalake/ already present; skipping unzip."
            exit 0
          fi

          echo "No datalake/ found; unpacking bootstrap ZIP…"
          if   [ -f "datalake-backfill.zip" ]; then ZIP_NAME="datalake-backfill.zip"
          elif [ -f "datalake-bootstrap.zip" ]; then ZIP_NAME="datalake-bootstrap.zip"
          else
            echo "❌ Neither datalake-backfill.zip nor datalake-bootstrap.zip found in repo root."
            exit 0
          fi

          sudo apt-get update -y >/dev/null 2>&1 || true
          sudo apt-get install -y unzip jq >/dev/null 2>&1 || true

          unzip -o "$ZIP_NAME" -d .
          echo "Archive contents (first 100 lines):"
          unzip -l "$ZIP_NAME" | sed -n '1,100p'

          if [ -d "datalake" ]; then
            echo "Archive already contained datalake/."
          else
            if [ -d "NIFTY500_ProPro/datalake" ]; then
              mv NIFTY500_ProPro/datalake ./ && rm -rf NIFTY500_ProPro
            elif [ -d "datalake-backfill/datalake" ]; then
              mv datalake-backfill/datalake ./ && rm -rf datalake-backfill
            elif [ -d "datalake-bootstrap/datalake" ]; then
              mv datalake-bootstrap/datalake ./ && rm -rf datalake-bootstrap
            else
              mkdir -p datalake
              [ -f "daily_equity.parquet" ] && mv daily_equity.parquet datalake/
              [ -f "daily_equity.csv" ]     && mv daily_equity.csv datalake/
              [ -f "manifest.txt" ]         && mv manifest.txt datalake/
              [ -f "universe.csv" ]         && mv universe.csv datalake/
              if [ -d "per_symbol" ]; then
                mkdir -p datalake/per_symbol
                mv per_symbol/* datalake/per_symbol/ || true
                rmdir per_symbol || true
              fi
            fi
          fi

          echo "✅ Final datalake tree:"
          ls -la datalake || true
          [ -d datalake/per_symbol ] && (echo "== datalake/per_symbol ==" && ls -la datalake/per_symbol | head -n 50)

      # ---------- Auto-fix: convert ALL relative imports under src/ ----------
      # This guarantees we won't hit "attempted relative import" errors.
      # You can remove this step later once everything is clean in the repo.
      - name: One-shot fix — convert ALL relative imports under src/
        run: |
          echo "Before:"
          grep -Rne '^[[:space:]]*from[[:space:]]+\.' src || true
          grep -Rne '^[[:space:]]*import[[:space:]]+\.' src || true

          while IFS= read -r -d '' f; do
            sed -i -E 's/^[[:space:]]*from[[:space:]]+\.\s*([A-Za-z0-9_\.]+)[[:space:]]+import/from \1 import/g' "$f"
            sed -i -E 's/^[[:space:]]*import[[:space:]]+\.\s*([A-Za-z0-9_\.]+)/import \1/g' "$f"
          done < <(find src -type f -name "*.py" -print0)

          echo "After:"
          grep -Rne '^[[:space:]]*from[[:space:]]+\.' src || echo "✅ no relative 'from .' imports"
          grep -Rne '^[[:space:]]*import[[:space:]]+\.' src || echo "✅ no relative 'import .' imports"

      - name: Show tree (sanity)
        run: |
          echo "== repo root =="
          ls -la
          echo "== src =="
          ls -la src || true

      - name: Setup Python
        uses: actions/setup-python@v5
        with:
          python-version: "3.11"

      - name: Install deps (non-fatal)
        run: |
          python -m pip install --upgrade pip
          if [ -f requirements.txt ]; then pip install -r requirements.txt; fi
          pip install yfinance ta requests pandas pyarrow || true

      # ---------- Self-Audit so you can see wiring status in logs ----------
      - name: Self-Audit — planned features & wiring
        env:
          PYTHONPATH:  ${{ github.workspace }}/src
        run: |
          python - <<'PY'
          import os, sys, json, importlib, glob, datetime as dt
          sys.path.append("src")
          out = {"when_utc": dt.datetime.utcnow().isoformat()+"Z"}
          out["datalake_present"] = os.path.isdir("datalake")
          out["daily_equity_present"] = os.path.exists("datalake/daily_equity.parquet") or os.path.exists("datalake/daily_equity.csv")
          out["per_symbol_count"] = len(glob.glob("datalake/per_symbol/*.csv"))
          errs = {}
          for m in ["config","pipeline","regime","feature_rules","indicators","options_executor","report_eod","report_periodic","kill_switch","telegram"]:
              try: importlib.import_module(m)
              except Exception as e: errs[m] = repr(e)
          out["import_errors"] = errs
          try:
              import config
              out["feature_flags"] = config.CONFIG.get("features", {})
              out["sector_cap_enabled"] = bool(config.CONFIG.get("selection",{}).get("sector_cap_enabled", False))
              out["notify_cfg"] = config.CONFIG.get("notify", {})
              out["options_cfg_present"] = "options" in config.CONFIG
          except Exception as e:
              out["config_error"] = repr(e)
          print("=== SELF-AUDIT JSON ===")
          print(json.dumps(out, indent=2))
          PY

      # ---------- Run your pipeline & write reports ----------
      - name: Run pipeline (defensive, never fail the job)
        env:
          TG_BOT_TOKEN: ${{ secrets.TG_BOT_TOKEN }}
          TG_CHAT_ID:  ${{ secrets.TG_CHAT_ID }}
          PYTHONPATH:  ${{ github.workspace }}/src
        run: |
          python - <<'PY'
          import sys, traceback
          from pathlib import Path
          if Path("src").exists(): sys.path.append("src")
          def safe(label, fn):
              print(f">> {label}")
              try:
                  fn()
                  print(f"<< {label} OK")
              except Exception as e:
                  print(f"<< {label} ERROR: {e}")
                  traceback.print_exc()
          try:
              from entrypoints import daily_update, eod_task, periodic_reports_task, after_run_housekeeping
              from pipeline import run_paper_session
          except Exception as e:
              print("Import error in main run:", e)
              traceback.print_exc()
              raise SystemExit(0)
          safe("daily_update()", lambda: daily_update())
          safe("run_paper_session()", lambda: run_paper_session(top_k=5))
          safe("eod_task()", lambda: eod_task())
          safe("periodic_reports_task()", lambda: periodic_reports_task())
          safe("after_run_housekeeping()", lambda: after_run_housekeeping())
          raise SystemExit(0)
          PY

      # ---------- Publish reports so you can download ----------
      - name: List reports (sanity)
        run: |
          echo "== reports =="
          ls -la reports || true
          echo "== killswitch =="
          ls -la datalake/killswitch_state.csv || true

      - name: Upload reports artifact
        if: always()
        uses: actions/upload-artifact@v4
        with:
          name: reports_bundle
          path: |
            reports/**
            datalake/killswitch_state.csv
          if-no-files-found: warn
